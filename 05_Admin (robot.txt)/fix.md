<h1>Some fixes for robots.txt</h1>

<ul>
<li> Disallow Directories, Not Specific Pages</li>
<li>If you disallow a directory, the nefarious person or robot might still be able to find the ‘hidden’ pages within the directory via brute force or
 the inurl search operator but the exact map of the pages won’t be laid out for them.</li>
<li>Use Noindex, Not Disallow, for Pages That Need to Be Private yet Publicly Accessible</li>
</ul>