<h1>Impacts of robot.txt</h1>


<h2>Security risk of Robots.txt</h2>
<ul>
<li>Site owners often in development sprints accidentally trigger the default robots.txt file which can then list a disallow line that blocks all site content.</li>
<li>The robots.txt file isn’t a hard directive, it is merely a suggestion. Good robots like Googlebot respect the directives in the file.
    Bad robots, though, may completely ignore it or worse. In fact, some nefarious robots and penetration test robots specifically look for robots.txt files for the very purpose of visiting the disallowed site sections because the robots.txt file’s disallow list can serve as a map. It is the first, most obvious place to look.
</li>
<h3>Not having a robots.txt</h3>
<p>How this can become an issue:</p>
<li>Having a robots.txt file is a recommended best practice for sites to add a level of control to the content and files that Google can crawl and index. Not having one simply means that Google will crawl and index all content.</li>
</ul><br/><br/>






<h2>Side notes</h2>

<h2>What is a robots.txt file?</h2>

<ul>
<li>The robots.txt file is used to tell web crawlers and other well-meaning robots a few things about the structure of a website. </li>
<li>It is openly accessible and can also be read and understood quickly and easily by humans.</li>

<h2>What is a robot you may ask</h2>
<li>An Internet bot, web robot, robot or simply bot, is a software application that runs automated tasks (scripts) over the Internet</li>
<li>Typically, bots perform tasks that are simple and repetitive, much faster than a person could.</li>
<li>The most extensive use of bots is for web crawling, in which an automated script fetches, analyzes and files information from web servers.</li>

</ul>




